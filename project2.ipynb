{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02285e6",
   "metadata": {
    "id": "a02285e6"
   },
   "source": [
    "# Starter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc5329",
   "metadata": {
    "id": "bdcc5329"
   },
   "source": [
    "Install and import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
   "metadata": {
    "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
    "ExecuteTime": {
     "end_time": "2025-04-15T15:13:51.654929Z",
     "start_time": "2025-04-15T15:13:03.974843Z"
    }
   },
   "source": [
    "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
    "!pip install nvidia-ml-py3"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.16.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.45.5-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.16-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Collecting rich (from trl)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp39-cp39-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp39-cp39-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.19.0-cp39-cp39-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=2.0.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 10.4/10.4 MB 107.7 MB/s eta 0:00:00\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
      "Downloading trl-0.16.1-py3-none-any.whl (336 kB)\n",
      "Using cached bitsandbytes-0.45.5-py3-none-win_amd64.whl (75.4 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.16-cp39-cp39-win_amd64.whl (442 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl (25.5 MB)\n",
      "   ---------------------------------------- 0.0/25.5 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 24.1/25.5 MB 117.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.5/25.5 MB 100.8 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 144.9 MB/s eta 0:00:00\n",
      "Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---- ---------------------------------- 24.9/204.1 MB 121.4 MB/s eta 0:00:02\n",
      "   --------- ----------------------------- 48.8/204.1 MB 119.4 MB/s eta 0:00:02\n",
      "   ------------- ------------------------- 73.1/204.1 MB 119.7 MB/s eta 0:00:02\n",
      "   ------------------ -------------------- 97.0/204.1 MB 119.1 MB/s eta 0:00:01\n",
      "   ---------------------- --------------- 120.8/204.1 MB 118.7 MB/s eta 0:00:01\n",
      "   -------------------------- ----------- 143.9/204.1 MB 117.8 MB/s eta 0:00:01\n",
      "   ---------------------------- --------- 153.6/204.1 MB 107.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 159.9/204.1 MB 98.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 167.5/204.1 MB 91.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 177.7/204.1 MB 86.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 193.5/204.1 MB 85.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 86.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 78.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 94.1 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp39-cp39-win_amd64.whl (51 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading multidict-6.4.3-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp39-cp39-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.19.0-cp39-cp39-win_amd64.whl (93 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, xxhash, tqdm, sympy, safetensors, regex, pyarrow, propcache, networkx, multidict, mdurl, fsspec, frozenlist, filelock, dill, async-timeout, aiohappyeyeballs, yarl, torch, multiprocess, markdown-it-py, huggingface-hub, aiosignal, tokenizers, rich, bitsandbytes, aiohttp, accelerate, transformers, peft, datasets, trl, evaluate\n",
      "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 filelock-3.18.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.30.2 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.2.1 peft-0.15.1 propcache-0.3.1 pyarrow-19.0.1 regex-2024.11.6 rich-14.0.0 safetensors-0.5.3 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 trl-0.16.1 xxhash-3.5.0 yarl-1.19.0\n",
      "Collecting nvidia-ml-py3\n",
      "  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: nvidia-ml-py3\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19219 sha256=20055a82c003afa0fb9c7a6e2331d7c22c75bd9aafe74b55aad57cfcb6b9fdd1\n",
      "  Stored in directory: c:\\users\\frank\\appdata\\local\\pip\\cache\\wheels\\f6\\d8\\b0\\15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "Successfully built nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3\n",
      "Successfully installed nvidia-ml-py3-7.352.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
   "metadata": {
    "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
    "ExecuteTime": {
     "end_time": "2025-04-21T15:03:19.026100Z",
     "start_time": "2025-04-21T15:03:10.570953Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "59d6e377",
   "metadata": {
    "id": "59d6e377"
   },
   "source": [
    "## Load Tokenizer and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
   "metadata": {
    "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
    "ExecuteTime": {
     "end_time": "2025-04-21T15:03:22.473470Z",
     "start_time": "2025-04-21T15:03:20.254760Z"
    }
   },
   "source": [
    "base_model = 'roberta-base'\n",
    "\n",
    "dataset = load_dataset('ag_news', split='train')\n",
    "dataset_test = load_dataset('ag_news', split='test')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T15:09:00.721032Z",
     "start_time": "2025-04-21T15:08:13.141091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Data cleaning\n",
    "def clean_text(example):\n",
    "    text = example[\"text\"]\n",
    "    \n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # 1. Lowercase\n",
    "    # text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "    # 3. Remove numbers\n",
    "    #text = re.sub(r'[^a-zA/-Z\\s]', '', text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # 4. Remove stopwords\n",
    "    # words = text.split()\n",
    "    # words = [word for word in words if word not in stop_words]\n",
    "    # more_stopwords = ['href', 'lt', 'gt', 'ii', 'iii', 'ie', 'quot', 'com']\n",
    "    # words = [word for word in words if word not in more_stopwords]\n",
    "    # stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    return {\"text\": text} #\" \".join(words)\n",
    "\n",
    "dataset_cleaned_train = dataset.map(clean_text)\n",
    "dataset_cleaned_test = dataset_test.map(clean_text)\n"
   ],
   "id": "56e62ea6838b6fd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\frank\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Map: 100%|██████████| 120000/120000 [00:44<00:00, 2684.49 examples/s]\n",
      "Map: 100%|██████████| 7600/7600 [00:02<00:00, 2666.20 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T22:03:04.705394Z",
     "start_time": "2025-04-18T22:02:06.524409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "def length_filter(example):\n",
    "    # Calculate the number of tokens\n",
    "    tokenized_input = tokenizer(example[\"text\"], truncation=False)\n",
    "    token_length = len(tokenized_input[\"input_ids\"]) \n",
    "    return 10 <= token_length <= 256\n",
    "\n",
    "dataset_cleaned_train = dataset_cleaned_train.filter(length_filter)\n",
    "dataset_cleaned_test = dataset_cleaned_test.filter(length_filter)\n",
    "print(f\"Train dataset length: {len(dataset_cleaned_train)}\")\n",
    "print(f\"Test dataset length: {len(dataset_cleaned_test)}\")"
   ],
   "id": "7b78623de7931cfc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 120000/120000 [00:51<00:00, 2321.87 examples/s]\n",
      "Filter: 100%|██████████| 7600/7600 [00:03<00:00, 1975.34 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 119998\n",
      "Test dataset length: 7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:20:17.466098Z",
     "start_time": "2025-04-18T15:19:09.927608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding=True,max_length=128)\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset_cleaned_train.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset_test = dataset_cleaned_test.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
    "tokenized_dataset_test = tokenized_dataset_test.rename_column(\"label\", \"labels\")"
   ],
   "id": "a69d1b9beca0f297",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [00:59<00:00, 2018.50 examples/s]\n",
      "Map: 100%|██████████| 7600/7600 [00:05<00:00, 1476.77 examples/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
   "metadata": {
    "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:20:38.054374Z",
     "start_time": "2025-04-18T15:20:38.047394Z"
    }
   },
   "source": [
    "# Extract the number of classess and their names\n",
    "num_labels = dataset_cleaned_train.features['label'].num_classes\n",
    "class_names = dataset_cleaned_train.features[\"label\"].names\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")\n",
    "\n",
    "# Create an id2label mapping\n",
    "# We will need this for our classifier.\n",
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c9e24afd",
   "metadata": {
    "id": "c9e24afd"
   },
   "source": [
    "## Load Pre-trained Model\n",
    "Set up config for pretrained model and download it from hugging face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265839d-a088-4693-8474-862641de11ed",
   "metadata": {
    "id": "f265839d-a088-4693-8474-862641de11ed"
   },
   "source": [
    "## Anything from here on can be modified"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7413430-be57-482b-856e-36bd4ba799df",
   "metadata": {
    "id": "e7413430-be57-482b-856e-36bd4ba799df",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:21:11.829089Z",
     "start_time": "2025-04-18T15:21:11.814067Z"
    }
   },
   "source": [
    "# Split the original training set\n",
    "# split_datasets = tokenized_dataset.train_test_split(test_size=640, seed=42)\n",
    "# train_dataset = split_datasets['train']\n",
    "# eval_dataset = split_datasets['test']\n",
    "train_dataset = tokenized_dataset\n",
    "eval_dataset = tokenized_dataset_test"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "652452e3",
   "metadata": {
    "id": "652452e3"
   },
   "source": [
    "## Setup LoRA Config\n",
    "Setup PEFT config and get peft model for finetuning"
   ]
  },
  {
   "metadata": {
    "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:21:13.919852Z",
     "start_time": "2025-04-18T15:21:13.737844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    base_model,\n",
    "    id2label=id2label)\n",
    "model"
   ],
   "id": "262a8416-a59c-4ea1-95d9-0b1f81d6094c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:21:20.308941Z",
     "start_time": "2025-04-18T15:21:20.094114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# peft_model.unload() \n",
    "lora_configs = [\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.0.attention.self.query\",\n",
    "            \"layer.0.attention.self.value\",\n",
    "            \"layer.0.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 8,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.1.attention.self.query\",\n",
    "            \"layer.1.attention.self.value\",\n",
    "            \"layer.1.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 4,\n",
    "        \"lora_alpha\": 8,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.2.attention.self.query\",\n",
    "            \"layer.2.attention.self.value\",\n",
    "            \"layer.2.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.3.attention.self.query\",\n",
    "            \"layer.3.attention.self.value\",\n",
    "            \"layer.3.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.4.attention.self.query\",\n",
    "            \"layer.4.attention.self.value\",\n",
    "            \"layer.4.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.5.attention.self.query\",\n",
    "            \"layer.5.attention.self.value\",\n",
    "            \"layer.5.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.6.attention.self.query\",\n",
    "            \"layer.6.attention.self.value\",\n",
    "            \"layer.6.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.7.attention.self.query\",\n",
    "            \"layer.7.attention.self.value\",\n",
    "            \"layer.7.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.8.attention.self.query\",\n",
    "            \"layer.8.attention.self.value\",\n",
    "            \"layer.8.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.9.attention.self.query\",\n",
    "            \"layer.9.attention.self.value\",\n",
    "            \"layer.9.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.10.attention.self.query\",\n",
    "            \"layer.10.attention.self.value\",\n",
    "            \"layer.10.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "    {\n",
    "        \"target_modules\": [\n",
    "            \"layer.11.attention.self.query\",\n",
    "            \"layer.11.attention.self.value\",\n",
    "            \"layer.11.attention.self.key\"\n",
    "        ],\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1\n",
    "    },\n",
    "   \n",
    "]\n",
    "\n",
    "\n",
    "for config in lora_configs:\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=config[\"target_modules\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias = 'none',\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        modules_to_save=[\"classifier\"]\n",
    "    )\n",
    "    peft_model = get_peft_model(model, lora_config)"
   ],
   "id": "df5e4734af2ed746",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "C:\\Users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:168: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
   "metadata": {
    "id": "a769f54e-05ad-4e3c-aae8-d00d1d9dfb2f",
    "ExecuteTime": {
     "end_time": "2025-04-16T21:45:05.472287Z",
     "start_time": "2025-04-16T21:45:05.460065Z"
    }
   },
   "source": [
    "# print(\"Trainable parameters:\")\n",
    "# for name, param in peft_model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name)"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
   "metadata": {
    "id": "da45f85c-b016-4c49-8808-6eafa7cb5d1b",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:21:26.036549Z",
     "start_time": "2025-04-18T15:21:26.019398Z"
    }
   },
   "source": [
    "print('PEFT Model')\n",
    "peft_model.print_trainable_parameters()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model\n",
      "trainable params: 999,172 || all params: 125,647,880 || trainable%: 0.7952\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "12284b58",
   "metadata": {
    "id": "12284b58"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
   "metadata": {
    "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:21:28.513558Z",
     "start_time": "2025-04-18T15:21:28.500505Z"
    }
   },
   "source": [
    "# To track evaluation accuracy during training\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': accuracy\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "768b4917-65de-4e55-ae7f-698e287535d4",
   "metadata": {
    "id": "768b4917-65de-4e55-ae7f-698e287535d4",
    "ExecuteTime": {
     "end_time": "2025-04-18T15:24:51.167328Z",
     "start_time": "2025-04-18T15:24:51.093218Z"
    }
   },
   "source": [
    "# Setup Training args\n",
    "output_dir = \"results\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4, #2e-4\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=12,\n",
    "    dataloader_num_workers=8,\n",
    "    use_cpu=False,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"labels\"],\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n",
    "\n",
    "def get_trainer(model):\n",
    "      return  Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          compute_metrics=compute_metrics,\n",
    "          train_dataset=train_dataset,\n",
    "          eval_dataset=eval_dataset,\n",
    "          data_collator=data_collator,\n",
    "      )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "9b848278",
   "metadata": {
    "id": "9b848278"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
   "metadata": {
    "id": "98d9d57d-b57f-4acc-80fb-fc5443e75515",
    "ExecuteTime": {
     "end_time": "2025-04-18T20:55:53.472064Z",
     "start_time": "2025-04-18T15:24:52.894126Z"
    }
   },
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "# peft_model.to(device)\n",
    "# \n",
    "# \n",
    "# peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
    "# # peft_lora_finetuning_trainer.train(resume_from_checkpoint=\"./outputs/checkpoint-1000\")\n",
    "# result = peft_lora_finetuning_trainer.train()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90000' max='90000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90000/90000 5:30:19, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.221259</td>\n",
       "      <td>0.933289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.218507</td>\n",
       "      <td>0.935658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.206328</td>\n",
       "      <td>0.940263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.205982</td>\n",
       "      <td>0.941053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.208451</td>\n",
       "      <td>0.942237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.202387</td>\n",
       "      <td>0.944868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.213131</td>\n",
       "      <td>0.944474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.204317</td>\n",
       "      <td>0.946316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.208751</td>\n",
       "      <td>0.949211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.948026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.217533</td>\n",
       "      <td>0.948289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.227991</td>\n",
       "      <td>0.947105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T14:49:52.191183Z",
     "start_time": "2025-04-17T14:49:50.993775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "teacher_model = RobertaForSequenceClassification.from_pretrained(\"roberta-large\",num_labels=4, id2label=id2label)\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, T=2.0):\n",
    "    soft_loss = F.kl_div(\n",
    "        F.log_softmax(student_logits / T, dim=-1),\n",
    "        F.softmax(teacher_logits / T, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T ** 2)\n",
    "\n",
    "\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss"
   ],
   "id": "7be33a015d79af7b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T14:53:03.077017Z",
     "start_time": "2025-04-17T14:52:16.244089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def evaluate_model_TS(model, eval_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, batch in enumerate(eval_dataloader):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        label = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "        \n",
    "        preds.extend(torch.argmax(output.logits, dim=-1).cpu().numpy())\n",
    "        labels.extend(label.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    print(f\"Evaluation Accuracy: {accuracy}\")\n",
    "    \n",
    "def preprocess_data(examples):\n",
    "    encodings = tokenizer(examples[\"text\"], truncation=False, padding=\"max_length\", max_length=128)\n",
    "    encodings[\"labels\"] = examples[\"label\"]\n",
    "    return encodings\n",
    "\n",
    "\n",
    "train_dataset = dataset_cleaned_train.map(preprocess_data, batched=True)\n",
    "eval_dataset = dataset_cleaned_test.map(preprocess_data, batched=True)\n"
   ],
   "id": "3129b50dee2ed4de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [00:41<00:00, 2890.41 examples/s]\n",
      "Map: 100%|██████████| 7600/7600 [00:03<00:00, 2328.83 examples/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:27:16.521138Z",
     "start_time": "2025-04-17T22:13:17.146041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "num_epochs = 20\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=True)\n",
    "peft_model = peft_model.to(device)\n",
    "optimizer = optim.AdamW(peft_model.parameters(), lr=5e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    peft_model.train()\n",
    "    total_loss = 0\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=inputs, attention_mask=attention_mask).logits\n",
    "\n",
    "        student_logits = peft_model(input_ids=inputs, attention_mask=attention_mask).logits\n",
    "\n",
    "        loss = distillation_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    #save model\n",
    "    peft_model.save_pretrained(f\"./results/checkpoint-{epoch+1}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "    evaluate_model_TS(peft_model, eval_dataloader)\n",
    "\n"
   ],
   "id": "b4d74cf6756dac87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [26:55<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4221450679620107\n",
      "Evaluation Accuracy: 0.9378947368421052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [26:57<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.4193462567090988\n",
      "Evaluation Accuracy: 0.9411842105263157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [26:58<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.41787741750876106\n",
      "Evaluation Accuracy: 0.9403947368421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [26:57<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.41572676539421083\n",
      "Evaluation Accuracy: 0.9356578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [26:58<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.4146461878299713\n",
      "Evaluation Accuracy: 0.9434210526315789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [27:03<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.41322443083922067\n",
      "Evaluation Accuracy: 0.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [58:34<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.4115957844336828\n",
      "Evaluation Accuracy: 0.9442105263157895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:06:29<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.4105415784200033\n",
      "Evaluation Accuracy: 0.9463157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:05:19<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.4092999792257945\n",
      "Evaluation Accuracy: 0.9413157894736842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:07:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.40818362799485525\n",
      "Evaluation Accuracy: 0.9447368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:06:42<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.40670950167973835\n",
      "Evaluation Accuracy: 0.9448684210526316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:05:23<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.40593271228472394\n",
      "Evaluation Accuracy: 0.9453947368421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:07:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.40477193874518075\n",
      "Evaluation Accuracy: 0.9468421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:06:58<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.40356317480405174\n",
      "Evaluation Accuracy: 0.9460526315789474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:05:25<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.4022507468779882\n",
      "Evaluation Accuracy: 0.9465789473684211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:07:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.4012085372130076\n",
      "Evaluation Accuracy: 0.9467105263157894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:06:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.4000289070447286\n",
      "Evaluation Accuracy: 0.9468421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [1:06:26<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.3994322786887487\n",
      "Evaluation Accuracy: 0.9453947368421053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 653/3750 [11:40<55:23,  1.07s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[44], line 36\u001B[0m\n\u001B[0;32m     33\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     34\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 36\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m#save model\u001B[39;00m\n\u001B[0;32m     38\u001B[0m peft_model\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./results/checkpoint-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:45:36.917066Z",
     "start_time": "2025-04-18T14:44:47.849458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# peft_model = PeftModel.from_pretrained(model, \"./results/checkpoint-5\")\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=True)\n",
    "evaluate_model_TS(peft_model, eval_dataloader)"
   ],
   "id": "1ac8dee778f817b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9468421052631579\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "5183be7e-514f-4e64-a6f4-314a827e6be5",
   "metadata": {
    "id": "5183be7e-514f-4e64-a6f4-314a827e6be5"
   },
   "source": [
    "## Evaluate Finetuned Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038198cf-0953-47e7-bd47-b073d05f8378",
   "metadata": {
    "id": "038198cf-0953-47e7-bd47-b073d05f8378"
   },
   "source": [
    "### Performing Inference on Custom Input\n",
    "Uncomment following functions for running inference on custom inputs"
   ]
  },
  {
   "cell_type": "code",
   "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
   "metadata": {
    "id": "f88ad420-3f46-4eff-9d71-0ce388163062",
    "ExecuteTime": {
     "end_time": "2025-04-17T00:40:18.773922Z",
     "start_time": "2025-04-17T00:40:18.769462Z"
    }
   },
   "source": [
    "def classify(model, tokenizer, text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = model(**inputs)\n",
    "\n",
    "    prediction = output.logits.argmax(dim=-1).item()\n",
    "\n",
    "    print(f'\\n Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
    "    return id2label[prediction]"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
   "metadata": {
    "id": "fc52bb94-5e13-4943-9225-a6d7fd053579",
    "ExecuteTime": {
     "end_time": "2025-04-17T01:01:09.677674Z",
     "start_time": "2025-04-17T01:01:09.584579Z"
    }
   },
   "source": [
    "classify( peft_model, tokenizer, \"Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\")\n",
    "classify( peft_model, tokenizer, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class: 0, Label: World, Text: Kederis proclaims innocence Olympic champion Kostas Kederis today left hospital ahead of his date with IOC inquisitors claiming his ...\n",
      "\n",
      " Class: 2, Label: Business, Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindlinand of ultra-cynics, are seeing green again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Business'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf",
   "metadata": {
    "id": "68a3e276-bf8c-4403-8a48-5ef19f2beccf"
   },
   "source": [
    "### Run Inference on eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
   "metadata": {
    "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:06:31.018751Z",
     "start_time": "2025-04-17T06:06:30.890784Z"
    }
   },
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import evaluate\n",
    "# from tqdm import tqdm\n",
    "# \n",
    "# def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
    "#     \"\"\"\n",
    "#     Evaluate a PEFT model on a dataset.\n",
    "# \n",
    "#     Args:\n",
    "#         inference_model: The model to evaluate.\n",
    "#         dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
    "#         labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
    "#                          If False, only predictions will be returned.\n",
    "#         batch_size (int): Batch size for inference.\n",
    "#         data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
    "# \n",
    "#     Returns:\n",
    "#         If labelled is True, returns a tuple (metrics, predictions)\n",
    "#         If labelled is False, returns the predictions.\n",
    "#     \"\"\"\n",
    "#     # Create the DataLoader\n",
    "#     eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "#     \n",
    "# \n",
    "#     inference_model.to(device)\n",
    "#     inference_model.eval()\n",
    "# \n",
    "#     all_predictions = []\n",
    "#     if labelled:\n",
    "#         metric = evaluate.load('accuracy')\n",
    "# \n",
    "#     # Loop over the DataLoader\n",
    "#     for batch in tqdm(eval_dataloader):\n",
    "#         # Move each tensor in the batch to the device\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         with torch.no_grad():\n",
    "#             outputs = inference_model(**batch)\n",
    "#         predictions = outputs.logits.argmax(dim=-1)\n",
    "#         all_predictions.append(predictions.cpu())\n",
    "# \n",
    "#         if labelled:\n",
    "#             # Expecting that labels are provided under the \"labels\" key.\n",
    "#             references = batch[\"labels\"]\n",
    "#             metric.add_batch(\n",
    "#                 predictions=predictions.cpu().numpy(),\n",
    "#                 references=references.cpu().numpy()\n",
    "#             )\n",
    "# \n",
    "#     # Concatenate predictions from all batches\n",
    "#     all_predictions = torch.cat(all_predictions, dim=0)\n",
    "# \n",
    "#     if labelled:\n",
    "#         eval_metric = metric.compute()\n",
    "#         print(\"Evaluation Metric:\", eval_metric)\n",
    "#         return eval_metric, all_predictions\n",
    "#     else:\n",
    "#         return all_predictions"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "809635a6-a2c7-4d09-8d60-ababd1815003",
   "metadata": {
    "id": "809635a6-a2c7-4d09-8d60-ababd1815003"
   },
   "source": [
    "# Check evaluation accuracy\n",
    "# dataset_cleaned_test = dataset_test.map(clean_text)\n",
    "# eval_dataset1 = dataset_cleaned_test.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "# eval_dataset1 = eval_dataset1.rename_column(\"label\", \"labels\")\n",
    "# peft_model = PeftModel.from_pretrained(model, \"./results/checkpoint-67500\")\n",
    "# _, _ = evaluate_model(peft_model, eval_dataset, True, 8, data_collator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "kMJgvV1ZnVhd"
   },
   "id": "kMJgvV1ZnVhd",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
   "metadata": {
    "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203"
   },
   "source": [
    "### Run Inference on unlabelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
   "metadata": {
    "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
    "ExecuteTime": {
     "end_time": "2025-04-18T14:52:47.145502Z",
     "start_time": "2025-04-18T14:52:47.127525Z"
    }
   },
   "source": [
    "#Load your unlabelled data\n",
    "import pickle\n",
    "import pandas as pd\n",
    "with open(\"test_unlabelled38.pkl\", \"rb\") as f:\n",
    "    unlabelled_dataset = pickle.load(f)\n",
    "# unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
    "unlabelled_dataset\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
   "metadata": {
    "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
    "ExecuteTime": {
     "end_time": "2025-04-17T06:11:09.005752Z",
     "start_time": "2025-04-17T06:10:19.180411Z"
    }
   },
   "source": [
    "# Run inference and save predictions\n",
    "# peft_model = PeftModel.from_pretrained(model, \"./results/checkpoint-67500\")\n",
    "# unlabelled_dataset = unlabelled_dataset.map(clean_text)\n",
    "# test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
    "# preds = evaluate_model_TS(peft_model, test_dataset, False, 8, data_collator)\n",
    "# df_output = pd.DataFrame({\n",
    "#     'ID': range(len(preds)),\n",
    "#     'Label': preds.numpy()  # or preds.tolist()\n",
    "# })\n",
    "# df_output.to_csv(os.path.join(output_dir,\"inference_output.csv\"), index=False)\n",
    "# print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\anaconda3\\envs\\python39\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:168: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 11337.45 examples/s]\n",
      "Map: 100%|██████████| 8000/8000 [00:04<00:00, 1851.86 examples/s]\n",
      "100%|██████████| 1000/1000 [00:43<00:00, 22.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to inference_output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:57:39.333727Z",
     "start_time": "2025-04-18T14:57:32.033045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data1(examples):\n",
    "    encodings = tokenizer(examples[\"text\"],  truncation=True, padding=True, max_length=128)\n",
    "    return encodings\n",
    "\n",
    "\n",
    "unlabelled_dataset = unlabelled_dataset.map(clean_text)\n",
    "taggle_dataset = unlabelled_dataset.map(preprocess_data1, batched=True,  remove_columns=[\"text\"])\n",
    "\n",
    "taggle_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "taggle_dataloader = DataLoader(taggle_dataset, batch_size=32, shuffle=False)\n",
    "peft_model = PeftModel.from_pretrained(model, \"./results/checkpoint-17\")\n",
    "peft_model.eval()\n"
   ],
   "id": "4bf95ebebb80a706",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:00<00:00, 10595.30 examples/s]\n",
      "Map: 100%|██████████| 8000/8000 [00:05<00:00, 1578.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-1): 2 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2-11): 10 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T14:58:30.932487Z",
     "start_time": "2025-04-18T14:57:39.403691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "preds = []\n",
    "for idx, batch in enumerate(taggle_dataloader):\n",
    "    inputs = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = peft_model(input_ids=inputs, attention_mask=attention_mask)\n",
    "    \n",
    "    preds.extend(torch.argmax(output.logits, dim=-1).cpu().numpy())\n",
    "    \n"
   ],
   "id": "58d56b612a381c5f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:01:05.103578Z",
     "start_time": "2025-04-18T15:01:05.073398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_output = pd.DataFrame({\n",
    "    'ID': range(len(preds)),\n",
    "    'Label': np.asarray(preds)  # or preds.tolist()\n",
    "})\n",
    "df_output.to_csv(os.path.join(output_dir,\"inference_output04181.csv\"), index=False)\n",
    "print(\"Inference complete. Predictions saved to inference_output.csv\")"
   ],
   "id": "505207ec6abd64ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions saved to inference_output.csv\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "caad0ae85b3273b6"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
